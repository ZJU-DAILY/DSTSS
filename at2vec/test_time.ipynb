{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e26a4-8cf8-4e50-b1a4-6305d1248c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "from at2vec import ctx, sp2id, EncoderDecoder\n",
    "# 配置\n",
    "if __name__ == '__main__':\n",
    "    torch.autograd.set_grad_enabled(False)\n",
    "    \n",
    "    # 训练集参数\n",
    "    ctx.min_x = -35.00002\n",
    "    ctx.max_x = 44.999763\n",
    "    ctx.min_y = 110.00003\n",
    "    ctx.max_y = 119.999954\n",
    "    ctx.min_ts = 1200083742000\n",
    "    ctx.max_ts = 1249975176000\n",
    "    ctx.num_ts_grids = (ctx.max_ts - ctx.min_ts) // ctx.ts_gap + 1\n",
    "    ctx.x_gap, ctx.y_gap = ((ctx.max_x - ctx.min_x) / ctx.num_x_grids,\n",
    "                            (ctx.max_y - ctx.min_y) / ctx.num_y_grids)\n",
    "    ctx.num_sp_grids = sp2id(ctx.max_x, ctx.max_y,\n",
    "                             ctx.min_x, ctx.min_y,\n",
    "                             ctx.max_x, ctx.max_y,\n",
    "                             ctx.x_gap, ctx.y_gap)\n",
    "    \n",
    "    ctx.test_tr_path = 'data/geolife-speed-4'\n",
    "    ctx.query_tr_path = 'data/geolife-speed-4-queries'\n",
    "    ctx.keywords_path = 'data/keywords.txt'\n",
    "    ctx.logging_path = 'data/test-time.log'\n",
    "    ctx.test_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c63106-bd16-4c2f-b30b-03ce5e6fd247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ChunkedTestDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, path: str, tr_len: int, raw2tr):\n",
    "        self.path = path\n",
    "        self.tr_len = tr_len\n",
    "        self.raw2tr = raw2tr\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with pd.read_csv(self.path, sep=\"\\t\",\n",
    "                         chunksize=ctx.test_batch_size * ctx.complete_tr_len, \n",
    "                         usecols=[0, 1, 2, 3, 4], header=None) as reader:\n",
    "            for data in reader:\n",
    "                raws = []\n",
    "                indexes = []\n",
    "                length = data.shape[0]\n",
    "                for i in range(0, length, self.tr_len):\n",
    "                    raws.append(data.iloc[i:i + self.tr_len])\n",
    "                    indexes.append(data.iloc[i, 0])\n",
    "                with Pool() as p:\n",
    "                    vectors = p.map(self.raw2tr, raws)\n",
    "                for index, vector in zip(indexes, vectors):\n",
    "                    yield index, vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e13a22-7c7d-4a20-9b1b-bb5f2e61703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.multiprocessing import Pool, set_start_method\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     set_start_method('spawn')\n",
    "\n",
    "def f(data):\n",
    "    return raw2tr(data[1]).cpu()\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, bare_dataset, raw2tr):\n",
    "        print(len(bare_dataset))\n",
    "        count = 0\n",
    "        for _ in iter(bare_dataset):\n",
    "            count += 1\n",
    "        print(count)\n",
    "        with Pool() as p:\n",
    "            self.vectors = p.map(f, bare_dataset)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (index, tr)\n",
    "        \"\"\"\n",
    "        tr = self.vectors[index]\n",
    "        return index, tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ace77-1b26-4f88-9e3f-9f5f7042cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "from functools import partial\n",
    "from at2vec import get_mat, PretrainModel, BareDataset, TrajectoryDataset, EncoderDecoder\n",
    "import torch\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 准备模型与数据\n",
    "    sp_model = PretrainModel(ctx.num_sp_grids, ctx.sp_len, torch.device('cpu'))\n",
    "    sp_model.load_state_dict(torch.load(ctx.sp_pretrain_model_path)['model'])\n",
    "    ts_model = PretrainModel(ctx.num_ts_grids, ctx.ts_len, torch.device('cpu'))\n",
    "    ts_model.load_state_dict(torch.load(ctx.ts_pretrain_model_path)['model'])\n",
    "    sm_model = Word2Vec.load(ctx.sm_pretrain_model_path)\n",
    "    \n",
    "    model = EncoderDecoder(ctx.sampled_tr_len, ctx.complete_tr_len, ctx.pt_len, ctx.hidden_len,\n",
    "                           ctx.num_sp_grids, ctx.num_ts_grids, len(sm_model.wv), ctx.device)\n",
    "    state = torch.load(ctx.at2vec_model_path)\n",
    "    model.load_state_dict(state['model'])\n",
    "    \n",
    "    raw2tr = partial(get_mat, sp_model=sp_model, ts_model=ts_model, sm_model=sm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431dee7-f046-4ff6-8c32-13cf4cd6afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    return param_size  # in bytes\n",
    "\n",
    "print(get_memory_usage(sp_model), get_memory_usage(ts_model), sm_model.estimate_memory())\n",
    "print(get_memory_usage(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95ab5c-a5a6-487b-97fa-081b7c3304ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab43ac9-fa5b-4126-87f2-c164e754eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(index: int, results):\n",
    "    hit_count = 0\n",
    "    min_accepted_idx = index // 50 * 50\n",
    "    max_accepted_idx = min_accepted_idx + 49\n",
    "    for _, idx in results:\n",
    "        if min_accepted_idx <= idx <= max_accepted_idx:\n",
    "            hit_count += 1\n",
    "    return hit_count / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65478a6-db08-481c-932d-13ac8d6d97da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "from datetime import datetime\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_dataset = ChunkedTestDataset(ctx.test_tr_path, ctx.complete_tr_len, raw2tr)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=ctx.test_batch_size)\n",
    "    \n",
    "    num_queries = 8\n",
    "    # dataset_range = [4096] * num_queries\n",
    "    dataset_range = [1876550, 1876550, 1876550, 1876550, 469150, 938300, 1407450, 1876550]\n",
    "    max_dataset_range = max(dataset_range)\n",
    "    ks = [1, 5, 10, 20, 50, 50, 50, 50]\n",
    "    \n",
    "    query_dataset = ChunkedTestDataset(ctx.query_tr_path, ctx.complete_tr_len, raw2tr)\n",
    "    \n",
    "    logging.basicConfig(filename=ctx.logging_path, format='%(message)s', level=logging.INFO)\n",
    "    logging.info(str(datetime.now()))\n",
    "\n",
    "    chosen_indexes = []\n",
    "    queues = [[]] * num_queries\n",
    "    chosen_vecs = []\n",
    "\n",
    "    query_it = iter(query_dataset)\n",
    "    for test_num in range(num_queries):\n",
    "        chosen_idx, chosen_tr = next(query_it)\n",
    "        chosen_indexes.append(int(chosen_idx))\n",
    "        chosen_tr = chosen_tr.to(ctx.device)\n",
    "        # chosen_vec: (1, hidden_len)\n",
    "        chosen_vec = model.get_rep_vector(chosen_tr).unsqueeze(0)\n",
    "        chosen_vecs.append(chosen_vec)\n",
    "        \n",
    "    time_move = 0.0\n",
    "    time_search = [0.0] * num_queries  # seconds\n",
    "    \n",
    "    batch_count = math.ceil(max_dataset_range/ctx.test_batch_size)\n",
    "    start_time = datetime.now()\n",
    "    for i, (indexes, trs) in enumerate(test_dataloader):\n",
    "        min_index = int(torch.min(indexes))\n",
    "        if min_index >= max_dataset_range:\n",
    "            break\n",
    "            \n",
    "        start = time.time()\n",
    "        trs = trs.to(ctx.device)\n",
    "        end = time.time()\n",
    "        time_move += end - start\n",
    "        # print(f'batch {i}')\n",
    "        \n",
    "        for j, chosen_vec in enumerate(chosen_vecs):\n",
    "            if (min_index >= dataset_range[j]):\n",
    "                continue\n",
    "            k = ks[j]\n",
    "            # print(f'query {j}')\n",
    "            start = time.time()\n",
    "            # indexes: (batch_size)\n",
    "            # trs: (batch_size, tr_len, pt_len)\n",
    "            # vecs: (batch_size, hidden_len)\n",
    "            vecs = model.get_rep_vector(trs)\n",
    "            if len(vecs.shape) == 1:\n",
    "                dists = torch.dist(chosen_vec.squeeze(), vecs.squeeze()).unsqueeze(0)\n",
    "            else:\n",
    "                dists = torch.cdist(chosen_vec, vecs).squeeze()\n",
    "            for x in range(indexes.shape[0]):\n",
    "                heapq.heappush(queues[j], (-dists[x].item(), indexes[x].item()))\n",
    "                if len(queues[j]) > k:\n",
    "                    heapq.heappop(queues[j])       \n",
    "            end = time.time()\n",
    "            time_search[j] += end - start\n",
    "        elapsed_time = datetime.now() - start_time\n",
    "        remaining_time = elapsed_time / ((i + 1) * ctx.test_batch_size) * (max_dataset_range - (i + 1) * ctx.test_batch_size)\n",
    "        print(f'Batch {i+1}/{batch_count}, {str(elapsed_time).split(\".\")[0]} < {str(remaining_time).split(\".\")[0]}')\n",
    "            \n",
    "    print('Results:')\n",
    "    for j, queue in enumerate(queues):\n",
    "        print(f'Query #{chosen_indexes[j]}')\n",
    "        results = sorted([(-k, v) for (k, v) in queue])\n",
    "        print(f'accuracy: {get_accuracy(chosen_indexes[j], results)}')\n",
    "    print(f'time move: {time_move}')\n",
    "    print(f'time search: {time_search}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
